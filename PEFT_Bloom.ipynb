{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyN3z9vpzHX7WEbrCEWMuHr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikjadhav2726/Research/blob/main/PEFT_Bloom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d16bf5ec-888b-4c76-a655-193fd4cc8a36",
          "showTitle": false,
          "title": ""
        },
        "id": "JechhJhhVaTA"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft==0.8.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets==2.14.5"
      ],
      "metadata": {
        "id": "9C0v2Hjl7EYX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "40s08K-W7HnM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bigscience/bloomz-560m\"\n",
        "#model_name=\"bigscience/bloom-1b1\"\n",
        "NUM_VIRTUAL_TOKENS = 4\n",
        "NUM_EPOCHS = 6"
      ],
      "metadata": {
        "id": "xmcMFtxM7KrD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "nAWsNj967NNW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100):\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        #temperature=0.2,\n",
        "        #top_p=0.95,\n",
        "        #do_sample=True,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=True, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "l36ZQzUd7QGQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompt = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\")\n",
        "foundational_outputs_prompt = get_outputs(foundational_model, input_prompt, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CogAE43g7TBB",
        "outputId": "4bb026f6-3aa7-48bd-e03e-0a51c1fecca2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"I want you to act as a motivational coach.  Don't be afraid of being challenged.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = tokenizer(\"There are two nice things that should matter to you:\", return_tensors=\"pt\")\n",
        "foundational_outputs_sentence = get_outputs(foundational_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX7yEGoa7V_b",
        "outputId": "f99ba87a-d289-40a0-8bc3-e89254ef7c11"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There are two nice things that should matter to you: the price and quality of your product.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "FeM6Tsle-vu8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_prompt = \"fka/awesome-chatgpt-prompts\"\n",
        "\n",
        "#Create the Dataset to create prompts.\n",
        "data_prompt = load_dataset(dataset_prompt)\n",
        "data_prompt = data_prompt.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "train_sample_prompt = data_prompt[\"train\"].select(range(50))\n"
      ],
      "metadata": {
        "id": "vWtQKa4g-y8l"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_sample_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "R2PA1z7A-2UQ",
        "outputId": "d5a5fd2f-053d-4a55-c3d4-1c9a6d781cd6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['act', 'prompt', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sample_prompt[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqSkPrhv-2kB",
        "outputId": "10b8492b-80ac-41e7-e12a-82c56a2026c2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'act': ['Linux Terminal'], 'prompt': ['I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd'], 'input_ids': [[44, 4026, 1152, 427, 1769, 661, 267, 104105, 28434, 17, 473, 2152, 4105, 49123, 530, 1152, 2152, 57502, 1002, 3595, 368, 28434, 3403, 6460, 17, 473, 4026, 1152, 427, 3804, 57502, 1002, 368, 28434, 10014, 14652, 2592, 19826, 4400, 10973, 15, 530, 16915, 4384, 17, 727, 1130, 11602, 184637, 17, 727, 1130, 4105, 49123, 35262, 473, 32247, 1152, 427, 727, 1427, 17, 3262, 707, 3423, 427, 13485, 1152, 7747, 361, 170205, 15, 707, 2152, 727, 1427, 1331, 55385, 5484, 14652, 6291, 999, 117805, 731, 29726, 1119, 96, 17, 2670, 3968, 9361, 632, 269, 42512]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sentences = load_dataset(\"Abirate/english_quotes\")\n",
        "\n",
        "data_sentences = dataset_sentences.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
        "train_sample_sentences = data_sentences[\"train\"].select(range(25))\n",
        "train_sample_sentences = train_sample_sentences.remove_columns(['author', 'tags'])"
      ],
      "metadata": {
        "id": "q4hhm27e-9sH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_sample_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "th7Hu0rP_DS8",
        "outputId": "35aab83d-93f3-4a84-ed3a-6ea3e60a6b81"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['quote', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 25\n",
              "})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
        "\n",
        "generation_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
        "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
        ")\n"
      ],
      "metadata": {
        "id": "r9HYqiXM_GKY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_prompt = get_peft_model(foundational_model, generation_config)\n",
        "print(peft_model_prompt.print_trainable_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qfzWAHc_JPa",
        "outputId": "490bdc15-510d-40a7-aa7d-408dd9dcac5b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_sentences = get_peft_model(foundational_model, generation_config)\n",
        "print(peft_model_sentences.print_trainable_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9o0aiwt_Ln7",
        "outputId": "8adec33b-fbaa-4cb1-facb-8c01e7d092ee"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "def create_training_arguments(path, learning_rate=0.0035, epochs=6):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
        "        use_cpu=True, # This is necessary for CPU clusters.\n",
        "        auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically\n",
        "        learning_rate= learning_rate, # Higher learning rate than full Fine-Tuning\n",
        "        num_train_epochs=epochs\n",
        "    )\n",
        "    return training_args"
      ],
      "metadata": {
        "id": "g-MCo2To_P5D"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "working_dir = \"./\"\n",
        "\n",
        "#Is best to store the models in separate folders.\n",
        "#Create the name of the directories where to store the models.\n",
        "output_directory_prompt =  os.path.join(working_dir, \"peft_outputs_prompt\")\n",
        "output_directory_sentences = os.path.join(working_dir, \"peft_outputs_sentences\")\n",
        "\n",
        "#Just creating the directoris if not exist.\n",
        "if not os.path.exists(working_dir):\n",
        "    os.mkdir(working_dir)\n",
        "if not os.path.exists(output_directory_prompt):\n",
        "    os.mkdir(output_directory_prompt)\n",
        "if not os.path.exists(output_directory_sentences):\n",
        "    os.mkdir(output_directory_sentences)\n"
      ],
      "metadata": {
        "id": "JPoGHz3M_Shn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args_prompt = create_training_arguments(output_directory_prompt, 0.003, NUM_EPOCHS)\n",
        "training_args_sentences = create_training_arguments(output_directory_sentences, 0.003, NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "wY7nzXtw_Uu3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "def create_trainer(model, training_args, train_dataset):\n",
        "    trainer = Trainer(\n",
        "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
        "        args=training_args, #The args for the training.\n",
        "        train_dataset=train_dataset, #The dataset used to tyrain the model.\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
        "    )\n",
        "    return trainer\n"
      ],
      "metadata": {
        "id": "1MNdfYUm_XKM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training first model.\n",
        "trainer_prompt = create_trainer(peft_model_prompt, training_args_prompt, train_sample_prompt)\n",
        "trainer_prompt.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "pd6uOWkT_aOw",
        "outputId": "4e346f37-6c27-47e5-a863-38277766480b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2/42 : < :, Epoch 0.14/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}