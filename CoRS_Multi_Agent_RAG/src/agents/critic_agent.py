"""
Critic Agent Implementation

This agent is responsible for evaluating the quality and faithfulness of
syntheses generated by Synthesizer Agents.
"""

import json
import logging
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from .base_agent import BaseAgent, AgentConfig

logger = logging.getLogger(__name__)


@dataclass
class CritiqueResult:
    """Result of a critique evaluation."""
    synthesis_id: str
    faithfulness_score: float
    clarity_score: float
    conciseness_score: float
    overall_score: float
    feedback: str
    issues: List[str]
    strengths: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        return {
            "synthesis_id": self.synthesis_id,
            "faithfulness_score": self.faithfulness_score,
            "clarity_score": self.clarity_score,
            "conciseness_score": self.conciseness_score,
            "overall_score": self.overall_score,
            "feedback": self.feedback,
            "issues": self.issues,
            "strengths": self.strengths
        }


class CriticAgent(BaseAgent):
    """
    Agent responsible for evaluating synthesis quality.
    
    The Critic Agent evaluates syntheses based on faithfulness to source
    material, clarity, conciseness, and overall quality. It provides
    detailed feedback and scores that are used by the consensus mechanism.
    """
    
    SYSTEM_PROMPT = """You are a meticulous quality evaluator specializing in assessing the faithfulness and quality of synthesized responses. Your role is to critically evaluate whether a synthesis accurately represents the source material without hallucination or distortion.

EVALUATION CRITERIA:

1. FAITHFULNESS (Most Important):
   - Does every claim in the synthesis have explicit support in the source context?
   - Are there any statements that go beyond what's stated in the sources?
   - Are there any factual errors or misrepresentations?
   - Are contradictions in sources properly acknowledged?

2. CLARITY:
   - Is the synthesis well-structured and easy to understand?
   - Is the language clear and professional?
   - Are complex concepts explained appropriately?

3. CONCISENESS:
   - Is the synthesis appropriately concise without being incomplete?
   - Are there unnecessary repetitions or verbose language?
   - Does it focus on the most important information?

SCORING SCALE (0.0 to 1.0):
- 1.0: Perfect - no issues found
- 0.8-0.9: Excellent - minor issues only
- 0.6-0.7: Good - some notable issues but generally sound
- 0.4-0.5: Fair - significant issues that affect quality
- 0.2-0.3: Poor - major issues, unreliable
- 0.0-0.1: Unacceptable - severe problems, completely unreliable

RESPONSE FORMAT:
Provide your evaluation as a JSON object:
{
    "faithfulness_score": 0.85,
    "clarity_score": 0.90,
    "conciseness_score": 0.80,
    "overall_score": 0.85,
    "feedback": "Overall assessment with specific observations",
    "issues": ["List of specific issues found"],
    "strengths": ["List of notable strengths"]
}

EXAMPLE:

Synthesis: "Renewable energy sources like solar and wind are environmentally friendly because they produce no emissions. They are also cost-effective in the long run."

Source Context: "Solar and wind power produce no direct carbon emissions during operation. Studies indicate renewable energy can reduce electricity costs over time due to lower operational expenses."

Evaluation:
{
    "faithfulness_score": 0.90,
    "clarity_score": 0.85,
    "conciseness_score": 0.90,
    "overall_score": 0.88,
    "feedback": "The synthesis accurately represents the source material with good clarity and conciseness. Minor improvement could be made by being more specific about 'no emissions' referring to 'no direct carbon emissions during operation'.",
    "issues": ["Could be more precise about 'no emissions' vs 'no direct carbon emissions during operation'"],
    "strengths": ["Accurate representation of source material", "Clear and concise language", "Good structure"]
}"""

    def __init__(self, agent_id: str = None, **kwargs):
        """
        Initialize the Critic Agent.
        
        Args:
            agent_id: Unique identifier for this agent
            **kwargs: Additional configuration parameters
        """
        if agent_id is None:
            agent_id = f"critic_{int(time.time())}"
        
        config = AgentConfig(
            agent_id=agent_id,
            agent_type="critic",
            temperature=0.1,  # Low temperature for consistent evaluation
            max_tokens=800,
            **kwargs
        )
        
        super().__init__(config)
        
        # Critic-specific settings
        self.faithfulness_weight = 0.5  # Higher weight for faithfulness
        self.clarity_weight = 0.3
        self.conciseness_weight = 0.2
        self.min_acceptable_score = 0.3
        self.detailed_feedback = True
    
    def get_system_prompt(self) -> str:
        """Get the system prompt for the Critic Agent."""
        return self.SYSTEM_PROMPT
    
    def process(self, input_data: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """
        Process a synthesis and provide critique.
        
        Args:
            input_data: Dictionary containing 'synthesis', 'context_documents', and 'sub_query'
            **kwargs: Additional processing parameters
            
        Returns:
            Dictionary with critique results and metadata
        """
        self.log_processing_start(input_data)
        
        try:
            # Validate input
            self.validate_input(input_data, ['synthesis', 'context_documents', 'sub_query'])
            synthesis = input_data['synthesis']
            context_documents = input_data['context_documents']
            sub_query = input_data['sub_query']
            
            # Extract synthesis content
            synthesis_text = synthesis.get('answer', '')
            synthesis_id = synthesis.get('synthesis_id', 'unknown')
            
            if not synthesis_text:
                error_result = {
                    "critique": {
                        "synthesis_id": synthesis_id,
                        "faithfulness_score": 0.0,
                        "clarity_score": 0.0,
                        "conciseness_score": 0.0,
                        "overall_score": 0.0,
                        "feedback": "No synthesis text provided for evaluation",
                        "issues": ["Empty synthesis text"],
                        "strengths": []
                    },
                    "success": False,
                    "error": "No synthesis text to evaluate"
                }
                self.log_processing_end(error_result, False)
                return error_result
            
            # Prepare context for evaluation
            context_text = self._prepare_context_for_evaluation(context_documents)
            
            # Create evaluation prompt
            prompt = self._create_evaluation_prompt(synthesis_text, context_text, sub_query)
            
            # Invoke LLM for evaluation
            llm_result = self.invoke_llm([self.create_human_message(prompt)])
            
            if not llm_result['success']:
                # Fallback evaluation
                fallback_critique = self._create_fallback_critique(synthesis_id, synthesis_text, context_documents)
                result = {
                    "critique": fallback_critique.to_dict(),
                    "sub_query": sub_query,
                    "success": True,
                    "critique_metadata": {
                        "used_fallback": True,
                        "error": llm_result.get('error', 'Unknown error')
                    }
                }
                self.log_processing_end(result, True)
                return result
            
            # Parse LLM evaluation response
            parsed_critique = self._parse_critique_response(llm_result['content'])
            
            if not parsed_critique:
                # Fallback evaluation
                fallback_critique = self._create_fallback_critique(synthesis_id, synthesis_text, context_documents)
                result = {
                    "critique": fallback_critique.to_dict(),
                    "sub_query": sub_query,
                    "success": True,
                    "critique_metadata": {
                        "used_fallback": True,
                        "parsing_failed": True,
                        "tokens_used": llm_result['tokens_used'],
                        "response_time": llm_result['response_time']
                    }
                }
            else:
                # Validate and enhance critique
                validated_critique = self._validate_critique_result(parsed_critique, synthesis_id)
                result = {
                    "critique": validated_critique.to_dict(),
                    "sub_query": sub_query,
                    "success": True,
                    "critique_metadata": {
                        "used_fallback": False,
                        "tokens_used": llm_result['tokens_used'],
                        "response_time": llm_result['response_time'],
                        "context_length": len(context_text),
                        "synthesis_length": len(synthesis_text)
                    }
                }
            
            self.log_processing_end(result, True)
            return result
            
        except Exception as e:
            error_result = {
                "critique": {
                    "synthesis_id": input_data.get('synthesis', {}).get('synthesis_id', 'unknown'),
                    "faithfulness_score": 0.0,
                    "clarity_score": 0.0,
                    "conciseness_score": 0.0,
                    "overall_score": 0.0,
                    "feedback": f"Error during evaluation: {str(e)}",
                    "issues": ["Technical error during evaluation"],
                    "strengths": []
                },
                "sub_query": input_data.get('sub_query', ''),
                "success": False,
                "error": str(e)
            }
            self.log_processing_end(error_result, False)
            return error_result
    
    def _prepare_context_for_evaluation(self, context_documents: List[Dict[str, Any]]) -> str:
        """
        Prepare context documents for evaluation.
        
        Args:
            context_documents: List of context documents
            
        Returns:
            Formatted context string
        """
        if not context_documents:
            return "No context documents provided."
        
        context_parts = []
        for i, doc in enumerate(context_documents):
            content = doc.get('content', '')
            if content:
                context_parts.append(f"Source {i+1}: {content}")
        
        return "\n\n".join(context_parts)
    
    def _create_evaluation_prompt(self, synthesis: str, context: str, sub_query: str) -> str:
        """
        Create the evaluation prompt for the LLM.
        
        Args:
            synthesis: Synthesis text to evaluate
            context: Source context
            sub_query: Original sub-query
            
        Returns:
            Formatted evaluation prompt
        """
        return self.format_prompt(
            """Sub-query: {sub_query}

Source Context:
{context}

Synthesis to Evaluate:
{synthesis}

Please evaluate this synthesis according to the criteria and format specified in the system prompt.""",
            sub_query=sub_query,
            context=context,
            synthesis=synthesis
        )
    
    def _parse_critique_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Parse the LLM critique response.
        
        Args:
            response: Raw LLM response
            
        Returns:
            Parsed critique dictionary or None if parsing failed
        """
        try:
            # Try to find JSON in the response
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                return None
            
            json_str = response[start_idx:end_idx]
            parsed = json.loads(json_str)
            
            # Validate required fields
            required_fields = ['faithfulness_score', 'clarity_score', 'conciseness_score', 'overall_score']
            if not all(field in parsed for field in required_fields):
                return None
            
            return parsed
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"Failed to parse critique response: {e}")
            return None
    
    def _validate_critique_result(self, critique: Dict[str, Any], synthesis_id: str) -> CritiqueResult:
        """
        Validate and enhance the critique result.
        
        Args:
            critique: Parsed critique dictionary
            synthesis_id: ID of the synthesis being evaluated
            
        Returns:
            Validated CritiqueResult object
        """
        # Ensure scores are within valid range [0.0, 1.0]
        faithfulness_score = max(0.0, min(1.0, critique.get('faithfulness_score', 0.5)))
        clarity_score = max(0.0, min(1.0, critique.get('clarity_score', 0.5)))
        conciseness_score = max(0.0, min(1.0, critique.get('conciseness_score', 0.5)))
        
        # Calculate weighted overall score
        overall_score = (
            faithfulness_score * self.faithfulness_weight +
            clarity_score * self.clarity_weight +
            conciseness_score * self.conciseness_weight
        )
        
        # Override overall score if provided and reasonable
        provided_overall = critique.get('overall_score', overall_score)
        if 0.0 <= provided_overall <= 1.0:
            # Use provided score if it's within reasonable range of calculated score
            if abs(provided_overall - overall_score) <= 0.2:
                overall_score = provided_overall
        
        return CritiqueResult(
            synthesis_id=synthesis_id,
            faithfulness_score=faithfulness_score,
            clarity_score=clarity_score,
            conciseness_score=conciseness_score,
            overall_score=overall_score,
            feedback=critique.get('feedback', 'No feedback provided'),
            issues=critique.get('issues', []),
            strengths=critique.get('strengths', [])
        )
    
    def _create_fallback_critique(self, 
                                 synthesis_id: str, 
                                 synthesis_text: str, 
                                 context_documents: List[Dict[str, Any]]) -> CritiqueResult:
        """
        Create a basic fallback critique when LLM evaluation fails.
        
        Args:
            synthesis_id: ID of the synthesis
            synthesis_text: Synthesis text
            context_documents: Context documents
            
        Returns:
            Basic CritiqueResult
        """
        # Simple heuristic-based evaluation
        issues = []
        strengths = []
        
        # Basic length check
        if len(synthesis_text) < 20:
            issues.append("Synthesis is very short")
        elif len(synthesis_text) > 500:
            issues.append("Synthesis may be too verbose")
        else:
            strengths.append("Appropriate length")
        
        # Check for evidence citation
        if any(word in synthesis_text.lower() for word in ['document', 'source', 'according to']):
            strengths.append("References source material")
        else:
            issues.append("No clear reference to source material")
        
        # Basic confidence assessment
        if not context_documents:
            faithfulness_score = 0.2
            issues.append("No context documents to verify against")
        else:
            faithfulness_score = 0.5  # Neutral score when we can't evaluate properly
        
        clarity_score = 0.6  # Assume reasonable clarity
        conciseness_score = 0.6  # Assume reasonable conciseness
        
        overall_score = (
            faithfulness_score * self.faithfulness_weight +
            clarity_score * self.clarity_weight +
            conciseness_score * self.conciseness_weight
        )
        
        return CritiqueResult(
            synthesis_id=synthesis_id,
            faithfulness_score=faithfulness_score,
            clarity_score=clarity_score,
            conciseness_score=conciseness_score,
            overall_score=overall_score,
            feedback="Fallback evaluation due to technical issues. Scores are estimated.",
            issues=issues,
            strengths=strengths
        )
    
    def batch_evaluate(self, syntheses: List[Dict[str, Any]], 
                      context_documents: List[Dict[str, Any]], 
                      sub_query: str) -> List[Dict[str, Any]]:
        """
        Evaluate multiple syntheses in batch.
        
        Args:
            syntheses: List of syntheses to evaluate
            context_documents: Context documents
            sub_query: Original sub-query
            
        Returns:
            List of critique results
        """
        results = []
        
        for synthesis in syntheses:
            input_data = {
                'synthesis': synthesis,
                'context_documents': context_documents,
                'sub_query': sub_query
            }
            
            result = self.process(input_data)
            results.append(result)
        
        return results
    
    def get_critique_stats(self) -> Dict[str, Any]:
        """
        Get statistics about critique performance.
        
        Returns:
            Dictionary with critique statistics
        """
        return {
            "agent_id": self.agent_id,
            "faithfulness_weight": self.faithfulness_weight,
            "clarity_weight": self.clarity_weight,
            "conciseness_weight": self.conciseness_weight,
            "min_acceptable_score": self.min_acceptable_score,
            "total_critiques": self.metrics.total_calls,
            "success_rate": self.metrics.success_rate,
            "avg_response_time": self.metrics.avg_response_time,
            "reputation_score": self.metrics.reputation_score
        }
    
    def update_evaluation_weights(self,
                                 faithfulness_weight: float = None,
                                 clarity_weight: float = None,
                                 conciseness_weight: float = None):
        """
        Update evaluation weights.
        
        Args:
            faithfulness_weight: Weight for faithfulness score
            clarity_weight: Weight for clarity score
            conciseness_weight: Weight for conciseness score
        """
        # Ensure weights are provided and sum to 1.0
        weights = [
            faithfulness_weight or self.faithfulness_weight,
            clarity_weight or self.clarity_weight,
            conciseness_weight or self.conciseness_weight
        ]
        
        total_weight = sum(weights)
        if total_weight > 0:
            self.faithfulness_weight = weights[0] / total_weight
            self.clarity_weight = weights[1] / total_weight
            self.conciseness_weight = weights[2] / total_weight
            
            logger.info(f"Updated evaluation weights for agent {self.agent_id}: "
                       f"faithfulness={self.faithfulness_weight:.2f}, "
                       f"clarity={self.clarity_weight:.2f}, "
                       f"conciseness={self.conciseness_weight:.2f}")
    
    def analyze_critique_patterns(self, critiques: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyze patterns in critique results.
        
        Args:
            critiques: List of critique results
            
        Returns:
            Dictionary with pattern analysis
        """
        if not critiques:
            return {"error": "No critiques provided for analysis"}
        
        # Extract scores
        faithfulness_scores = [c['critique']['faithfulness_score'] for c in critiques if 'critique' in c]
        clarity_scores = [c['critique']['clarity_score'] for c in critiques if 'critique' in c]
        conciseness_scores = [c['critique']['conciseness_score'] for c in critiques if 'critique' in c]
        overall_scores = [c['critique']['overall_score'] for c in critiques if 'critique' in c]
        
        # Common issues
        all_issues = []
        for critique in critiques:
            if 'critique' in critique:
                all_issues.extend(critique['critique'].get('issues', []))
        
        issue_counts = {}
        for issue in all_issues:
            issue_counts[issue] = issue_counts.get(issue, 0) + 1
        
        return {
            "total_critiques": len(critiques),
            "average_scores": {
                "faithfulness": sum(faithfulness_scores) / len(faithfulness_scores) if faithfulness_scores else 0,
                "clarity": sum(clarity_scores) / len(clarity_scores) if clarity_scores else 0,
                "conciseness": sum(conciseness_scores) / len(conciseness_scores) if conciseness_scores else 0,
                "overall": sum(overall_scores) / len(overall_scores) if overall_scores else 0
            },
            "score_ranges": {
                "faithfulness": {"min": min(faithfulness_scores), "max": max(faithfulness_scores)} if faithfulness_scores else None,
                "clarity": {"min": min(clarity_scores), "max": max(clarity_scores)} if clarity_scores else None,
                "conciseness": {"min": min(conciseness_scores), "max": max(conciseness_scores)} if conciseness_scores else None,
                "overall": {"min": min(overall_scores), "max": max(overall_scores)} if overall_scores else None
            },
            "common_issues": dict(sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:5]),
            "quality_distribution": {
                "excellent": len([s for s in overall_scores if s >= 0.8]),
                "good": len([s for s in overall_scores if 0.6 <= s < 0.8]),
                "fair": len([s for s in overall_scores if 0.4 <= s < 0.6]),
                "poor": len([s for s in overall_scores if s < 0.4])
            }
        }


def create_critic_agent(agent_id: str = None, **kwargs) -> CriticAgent:
    """
    Factory function to create a Critic Agent.
    
    Args:
        agent_id: Optional agent identifier
        **kwargs: Additional configuration parameters
        
    Returns:
        Configured CriticAgent instance
    """
    return CriticAgent(agent_id=agent_id, **kwargs)